# COMPLETE BILINGUAL FINANCE RESEARCH HUB - FIXED VERSION
import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import numpy as np
import json
from datetime import datetime
import time
import re
import os

# ==================== PAGE CONFIG ====================
st.set_page_config(
    page_title="Finance Research Hub",
    page_icon="ğŸ“ˆ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ==================== BILINGUAL KEYWORD DATABASE ====================
FINANCE_KEYWORD_DATABASE = {
    "Computational Finance": {
        "keywords": [
            "deep learning", "neural networks", "machine learning", "AI", "artificial intelligence",
            "gradient descent", "backpropagation", "convolutional", "recurrent", "transformer",
            "PDE", "partial differential equation", "numerical methods", "finite difference", "finite element",
            "Monte Carlo", "simulation", "stochastic", "high-dimensional", "computational",
            "algorithm", "optimization", "parallel computing", "GPU", "CUDA",
            "æ·±åº¦å­¦ä¹ ", "ç¥ç»ç½‘ç»œ", "æœºå™¨å­¦ä¹ ", "äººå·¥æ™ºèƒ½", "AI",
            "æ¢¯åº¦ä¸‹é™", "åå‘ä¼ æ’­", "å·ç§¯", "å¾ªç¯ç¥ç»ç½‘ç»œ", "Transformer",
            "åå¾®åˆ†æ–¹ç¨‹", "æ•°å€¼æ–¹æ³•", "æœ‰é™å·®åˆ†", "æœ‰é™å…ƒ", "è’™ç‰¹å¡æ´›",
            "æ¨¡æ‹Ÿ", "éšæœº", "é«˜ç»´", "è®¡ç®—é‡‘è", "ç®—æ³•",
            "ä¼˜åŒ–", "å¹¶è¡Œè®¡ç®—", "é‡å­è®¡ç®—", "å¼ºåŒ–å­¦ä¹ ", "æ—¶é—´åºåˆ—é¢„æµ‹"
        ],
        "weight": 1.0,
        "color": "#667eea",
        "icon": "ğŸ’»"
    },
    
    "Mathematical Finance": {
        "keywords": [
            "stochastic calculus", "Ito", "Stratonovich", "Brownian motion", "martingale",
            "partial differential equation", "PDE", "Black-Scholes", "option pricing", "risk-neutral",
            "measure theory", "probability", "stochastic processes", "Levy processes", "jump diffusion",
            "Malliavin calculus", "Heston model", "SABR", "local volatility", "stochastic volatility",
            "éšæœºå¾®ç§¯åˆ†", "ä¼Šè—¤", "å¸ƒæœ—è¿åŠ¨", "é…", "åå¾®åˆ†æ–¹ç¨‹",
            "å¸ƒè±å…‹-æ–¯ç§‘å°”æ–¯", "æœŸæƒå®šä»·", "é£é™©ä¸­æ€§", "æµ‹åº¦è®º", "æ¦‚ç‡",
            "éšæœºè¿‡ç¨‹", "Levyè¿‡ç¨‹", "è·³è·ƒæ‰©æ•£", "Malliavinå¾®ç§¯åˆ†", "Hestonæ¨¡å‹",
            "å±€éƒ¨æ³¢åŠ¨ç‡", "éšæœºæ³¢åŠ¨ç‡", "æœ€ä¼˜åœæ­¢", "æœ€ä¼˜æ§åˆ¶", "åŠ¨æ€è§„åˆ’"
        ],
        "weight": 0.95,
        "color": "#f59e0b",
        "icon": "ğŸ“"
    },
    
    "Portfolio Management": {
        "keywords": [
            "portfolio optimization", "asset allocation", "diversification", "efficient frontier",
            "mean-variance", "Markowitz", "Black-Litterman", "risk parity", "minimum variance",
            "tactical asset allocation", "strategic asset allocation", "rebalancing", "turnover",
            "tracking error", "active share", "index tracking", "enhanced indexing",
            "factor investing", "smart beta", "risk factors", "style factors",
            "æŠ•èµ„ç»„åˆä¼˜åŒ–", "èµ„äº§é…ç½®", "åˆ†æ•£åŒ–", "æœ‰æ•ˆå‰æ²¿", "å‡å€¼æ–¹å·®",
            "é©¬ç§‘ç»´èŒ¨", "é£é™©å¹³ä»·", "æœ€å°æ–¹å·®", "æˆ˜æœ¯èµ„äº§é…ç½®", "æˆ˜ç•¥èµ„äº§é…ç½®",
            "å†å¹³è¡¡", "æ¢æ‰‹ç‡", "è·Ÿè¸ªè¯¯å·®", "ä¸»åŠ¨ä»½é¢", "æŒ‡æ•°è·Ÿè¸ª",
            "å› å­æŠ•èµ„", "æ™ºèƒ½è´å¡”", "é£é™©å› å­", "é£æ ¼å› å­", "å¯¹å†²åŸºé‡‘"
        ],
        "weight": 0.9,
        "color": "#10b981",
        "icon": "ğŸ“Š"
    },
    
    "Risk Management": {
        "keywords": [
            "value at risk", "VaR", "expected shortfall", "ES", "CVaR", "conditional value at risk",
            "stress testing", "scenario analysis", "backtesting", "historical simulation",
            "credit risk", "default risk", "counterparty risk", "credit value adjustment", "CVA",
            "market risk", "volatility risk", "interest rate risk", "currency risk",
            "é£é™©ä»·å€¼", "VaR", "é¢„æœŸæŸå¤±", "æ¡ä»¶é£é™©ä»·å€¼", "å‹åŠ›æµ‹è¯•",
            "æƒ…æ™¯åˆ†æ", "å›æµ‹", "å†å²æ¨¡æ‹Ÿ", "ä¿¡ç”¨é£é™©", "è¿çº¦é£é™©",
            "äº¤æ˜“å¯¹æ‰‹é£é™©", "ä¿¡ç”¨ä»·å€¼è°ƒæ•´", "å¸‚åœºé£é™©", "æ³¢åŠ¨ç‡é£é™©",
            "åˆ©ç‡é£é™©", "æ±‡ç‡é£é™©", "æµåŠ¨æ€§é£é™©", "èµ„é‡‘æµåŠ¨æ€§", "å¸‚åœºæµåŠ¨æ€§"
        ],
        "weight": 0.9,
        "color": "#8b5cf6",
        "icon": "âš ï¸"
    },
    
    "Green Finance": {
        "keywords": [
            "green finance", "green bonds", "green loans", "green credit", "sustainable finance",
            "environmental finance", "eco-finance", "green investment", "ESG investment",
            "environmental, social and governance", "green banking", "green insurance",
            "ç»¿è‰²é‡‘è", "ç»¿è‰²å€ºåˆ¸", "ç»¿è‰²è´·æ¬¾", "ç»¿è‰²ä¿¡è´·", "å¯æŒç»­é‡‘è",
            "ç¯å¢ƒé‡‘è", "ç”Ÿæ€é‡‘è", "ç»¿è‰²æŠ•èµ„", "ESGæŠ•èµ„", "ç¯å¢ƒç¤¾ä¼šæ²»ç†",
            "ç»¿è‰²é“¶è¡Œ", "ç»¿è‰²ä¿é™©", "ç»¿è‰²é‡‘èäº§å“", "ç»¿è‰²è¯åˆ¸", "ç»¿è‰²è½¬å‹é‡‘è",
            "ä½ç¢³é‡‘è", "å¾ªç¯ç»æµé‡‘è", "ç”Ÿç‰©å¤šæ ·æ€§é‡‘è", "è‡ªç„¶èµ„æœ¬",
            "ç»¿è‰²é‡‘èç§‘æŠ€", "å¯æŒç»­å‘å±•æŒ‚é’©è´·æ¬¾", "ç»¿è‰²æŠµæŠ¼è´·æ¬¾"
        ],
        "weight": 0.85,
        "color": "#22c55e",
        "icon": "ğŸŒ¿"
    },
    
    "Climate Finance": {
        "keywords": [
            "climate finance", "climate change finance", "climate risk finance", "climate adaptation finance",
            "climate mitigation finance", "carbon pricing", "carbon markets", "emissions trading",
            "carbon credits", "carbon offsets", "clean development mechanism", "CDM",
            "æ°”å€™é‡‘è", "æ°”å€™å˜åŒ–é‡‘è", "æ°”å€™é£é™©é‡‘è", "æ°”å€™é€‚åº”é‡‘è",
            "æ°”å€™å‡ç¼“é‡‘è", "ç¢³å®šä»·", "ç¢³å¸‚åœº", "ç¢³æ’æ”¾äº¤æ˜“",
            "ç¢³ä¿¡ç”¨", "ç¢³æŠµæ¶ˆ", "æ¸…æ´å‘å±•æœºåˆ¶", "æ°”å€™å€ºåˆ¸",
            "æ°”å€™åŸºé‡‘", "ç»¿è‰²æ°”å€™åŸºé‡‘", "é€‚åº”èèµ„", "å‡ç¼“èèµ„"
        ],
        "weight": 0.85,
        "color": "#0ea5e9",
        "icon": "ğŸŒ"
    },
    
    "Sustainable Finance": {
        "keywords": [
            "ESG", "environmental social governance", "sustainable investing", "responsible investing",
            "green bonds", "climate bonds", "sustainability-linked bonds", "social bonds",
            "sustainable development goals", "SDG finance", "social finance", "impact bonds",
            "ESG", "ç¯å¢ƒç¤¾ä¼šæ²»ç†", "å¯æŒç»­æŠ•èµ„", "è´£ä»»æŠ•èµ„", "ç¤¾ä¼šè´£ä»»æŠ•èµ„",
            "ç»¿è‰²å€ºåˆ¸", "æ°”å€™å€ºåˆ¸", "å¯æŒç»­å‘å±•æŒ‚é’©å€ºåˆ¸", "ç¤¾ä¼šå€ºåˆ¸",
            "å¯æŒç»­å‘å±•ç›®æ ‡", "SDGèèµ„", "ç¤¾ä¼šé‡‘è", "å½±å“åŠ›å€ºåˆ¸"
        ],
        "weight": 0.8,
        "color": "#10b981",
        "icon": "ğŸŒ±"
    },
    
    "FinTech & Blockchain": {
        "keywords": [
            "blockchain", "distributed ledger", "smart contracts", "Ethereum", "solidity",
            "cryptocurrency", "Bitcoin", "Ethereum", "DeFi", "decentralized finance",
            "stablecoins", "CBDC", "central bank digital currency", "digital currency",
            "åŒºå—é“¾", "åˆ†å¸ƒå¼è´¦æœ¬", "æ™ºèƒ½åˆçº¦", "ä»¥å¤ªåŠ", "åŠ å¯†è´§å¸",
            "æ¯”ç‰¹å¸", "å»ä¸­å¿ƒåŒ–é‡‘è", "ç¨³å®šå¸", "å¤®è¡Œæ•°å­—è´§å¸",
            "æ•°å­—è´§å¸", "ä»£å¸åŒ–", "éåŒè´¨åŒ–ä»£å¸", "è¯åˆ¸å‹ä»£å¸"
        ],
        "weight": 0.8,
        "color": "#6366f1",
        "icon": "ğŸ”—"
    },
    
    "Banking & Financial Institutions": {
        "keywords": [
            "commercial banks", "investment banks", "central banks", "bank regulation", "Basel",
            "capital adequacy", "liquidity coverage ratio", "LCR", "net stable funding ratio", "NSFR",
            "bank lending", "credit creation", "interbank market", "bank runs", "deposit insurance",
            "å•†ä¸šé“¶è¡Œ", "æŠ•èµ„é“¶è¡Œ", "ä¸­å¤®é“¶è¡Œ", "é“¶è¡Œç›‘ç®¡", "å·´å¡å°”åè®®",
            "èµ„æœ¬å……è¶³ç‡", "æµåŠ¨æ€§è¦†ç›–ç‡", "å‡€ç¨³å®šèµ„é‡‘æ¯”ä¾‹", "é“¶è¡Œä¿¡è´·",
            "ä¿¡ç”¨åˆ›é€ ", "é“¶è¡Œé—´å¸‚åœº", "é“¶è¡ŒæŒ¤å…‘", "å­˜æ¬¾ä¿é™©", "å½±å­é“¶è¡Œ",
            "é‡‘èä¸­ä»‹", "é“¶è¡Œç›ˆåˆ©èƒ½åŠ›", "ä¸è‰¯è´·æ¬¾", "é‡‘èç¨³å®š"
        ],
        "weight": 0.85,
        "color": "#8b4513",
        "icon": "ğŸ¦"
    },
    
    "Corporate Finance": {
        "keywords": [
            "capital structure", "Modigliani-Miller", "dividend policy", "payout policy", "share repurchase",
            "mergers and acquisitions", "M&A", "takeovers", "corporate governance", "board of directors",
            "agency theory", "principal-agent problem", "executive compensation", "CEO pay",
            "èµ„æœ¬ç»“æ„", "è«è¿ªåˆ©äºšå°¼-ç±³å‹’", "è‚¡åˆ©æ”¿ç­–", "æ´¾æ¯æ”¿ç­–", "è‚¡ç¥¨å›è´­",
            "å…¼å¹¶ä¸æ”¶è´­", "å¹¶è´­", "æ¥ç®¡", "å…¬å¸æ²»ç†", "è‘£äº‹ä¼š",
            "ä»£ç†ç†è®º", "å§”æ‰˜ä»£ç†é—®é¢˜", "é«˜ç®¡è–ªé…¬", "é¦–å¸­æ‰§è¡Œå®˜è–ªé…¬",
            "å…¬å¸æŠ•èµ„", "èµ„æœ¬é¢„ç®—", "å‡€ç°å€¼", "å†…éƒ¨æ”¶ç›Šç‡"
        ],
        "weight": 0.8,
        "color": "#4169e1",
        "icon": "ğŸ¢"
    }
}

# ==================== DEBUG - CHECK FILES ====================
def check_files():
    """Check if required files exist"""
    st.sidebar.subheader("ğŸ” File Check")
    
    # Check Excel file
    excel_file = 'CNKI-20260104152201560.xls'
    if os.path.exists(excel_file):
        st.sidebar.success(f"âœ… {excel_file}")
        st.sidebar.caption(f"Size: {os.path.getsize(excel_file)} bytes")
    else:
        st.sidebar.error(f"âŒ {excel_file} not found")
    
    # Check JSON file
    json_file = 'research_papers.json'
    if os.path.exists(json_file):
        st.sidebar.success(f"âœ… {json_file}")
    else:
        st.sidebar.warning(f"âš ï¸ {json_file} not found")
    
    # Show current directory
    if st.sidebar.checkbox("Show directory contents"):
        st.sidebar.code(f"CWD: {os.getcwd()}")
        files = os.listdir('.')
        for f in files[:10]:  # Show first 10 files
            st.sidebar.write(f"  - {f}")

# ==================== UTILITY FUNCTIONS ====================
def calculate_category_scores(text, top_k=5):
    """Calculate classification scores based on keyword matching"""
    if not text:
        return []
    
    text_lower = text.lower()
    scores = {}
    
    for category, data in FINANCE_KEYWORD_DATABASE.items():
        score = 0
        matched_keywords = []
        
        for keyword in data['keywords']:
            if keyword.lower() in text_lower:
                score += 1
                matched_keywords.append(keyword)
        
        weighted_score = score * data['weight']
        
        if weighted_score > 0:
            scores[category] = {
                'score': weighted_score,
                'confidence': min(100, weighted_score * 8),
                'matched_keywords': matched_keywords[:10],
                'total_matches': len(matched_keywords),
                'icon': data['icon'],
                'color': data['color']
            }
    
    sorted_categories = sorted(scores.items(), key=lambda x: x[1]['score'], reverse=True)
    return sorted_categories[:top_k]

def enhanced_classify_with_confidence(text, top_k=5):
    """Enhanced classification function with keyword-based scoring"""
    category_scores = calculate_category_scores(text, top_k)
    
    results = []
    for category, data in category_scores:
        results.append({
            "category": category,
            "confidence": data['confidence'],
            "score": data['score'],
            "icon": data['icon'],
            "color": data['color'],
            "matched_keywords": data['matched_keywords'],
            "total_matches": data['total_matches']
        })
    
    if not results:
        default_categories = ["General Finance", "Banking & Financial Institutions", "Green Finance"]
        for category in default_categories[:top_k]:
            results.append({
                "category": category,
                "confidence": 20.0,
                "score": 2.0,
                "icon": FINANCE_KEYWORD_DATABASE.get(category, {}).get('icon', 'ğŸ“„'),
                "color": FINANCE_KEYWORD_DATABASE.get(category, {}).get('color', '#764ba2'),
                "matched_keywords": [],
                "total_matches": 0
            })
    
    return results

def enhanced_classification_for_cnki(title, keywords, category_code):
    """Enhanced classification specifically for CNKI papers"""
    text = f"{title} {' '.join(keywords)} {category_code}"
    
    chinese_keyword_mapping = {
        "ç»¿è‰²é‡‘è": "Green Finance",
        "ç»¿è‰²å€ºåˆ¸": "Green Finance",
        "ç»¿è‰²ä¿¡è´·": "Green Finance",
        "ç»¿è‰²æŠ•èµ„": "Green Finance",
        "ESG": "Green Finance",
        "æ°”å€™é‡‘è": "Climate Finance",
        "ç¢³é‡‘è": "Climate Finance",
        "ç¢³äº¤æ˜“": "Climate Finance",
        "ç¢³å¸‚åœº": "Climate Finance",
        "ç¢³æ’æ”¾": "Climate Finance",
        "ç¢³ä¸­å’Œ": "Climate Finance",
        "å•†ä¸šé“¶è¡Œ": "Banking & Financial Institutions",
        "é“¶è¡Œ": "Banking & Financial Institutions",
        "é“¶è¡Œä¸š": "Banking & Financial Institutions",
        "é‡‘èç§‘æŠ€": "FinTech & Blockchain",
        "æ•°å­—è´§å¸": "FinTech & Blockchain",
        "åŒºå—é“¾": "FinTech & Blockchain",
        "é£é™©ç®¡ç†": "Risk Management",
        "é£é™©": "Risk Management",
        "ä¿¡ç”¨é£é™©": "Risk Management",
        "æŠ•èµ„ç»„åˆ": "Portfolio Management",
        "èµ„äº§é…ç½®": "Portfolio Management",
        "é‡‘èå·¥ç¨‹": "Mathematical Finance",
        "é‡åŒ–é‡‘è": "Mathematical Finance",
        "é‡‘èæ•°å­¦": "Mathematical Finance",
    }
    
    for chinese_keyword, category in chinese_keyword_mapping.items():
        if chinese_keyword in text:
            return category
    
    categories = calculate_category_scores(text, top_k=1)
    if categories:
        return categories[0][0]
    
    return "General Finance"

# ==================== LOAD EXCEL DATA ====================
def load_excel_data(file_path):
    """Load and process Excel data from CNKI export"""
    try:
        # Try different Excel engines
        try:
            df = pd.read_excel(file_path, sheet_name=None, engine='openpyxl')
        except:
            try:
                df = pd.read_excel(file_path, sheet_name=None, engine='xlrd')
            except:
                st.error("Cannot read Excel file. Please install: pip install openpyxl xlrd")
                return []
        
        all_papers = []
        
        for sheet_name, sheet_df in df.items():
            sheet_df.columns = sheet_df.columns.str.strip()
            
            # Handle different sheet formats
            if 'Title-é¢˜å' in sheet_df.columns:
                for idx, row in sheet_df.iterrows():
                    if pd.isna(row.get('Title-é¢˜å')):
                        continue
                    
                    # Extract authors
                    authors_str = str(row.get('Author-ä½œè€…', ''))
                    authors = [author.strip() for author in authors_str.split(',') if author.strip()]
                    
                    # Extract keywords
                    keywords_str = str(row.get('å…³é”®è¯', ''))
                    keywords = []
                    if isinstance(keywords_str, str):
                        keywords = [kw.strip() for kw in keywords_str.split(';;') if kw.strip()]
                    
                    # Create paper object
                    paper = {
                        'title': str(row.get('Title-é¢˜å', '')),
                        'authors': authors,
                        'source': str(row.get('Source-æ–‡çŒ®æ¥æº', row.get('Source-æŠ¥çº¸å', ''))),
                        'year': int(row.get('Year-å¹´', 2024)) if not pd.isna(row.get('Year-å¹´')) else 2024,
                        'keywords': keywords,
                        'category_code': str(row.get('ä¸­å›¾åˆ†ç±»å·', '')),
                        'type': 'journal' if 'Source-æ–‡çŒ®æ¥æº' in row else 'newspaper',
                        'abstract': '',
                        'arxiv_id': f"CNKI_{sheet_name}_{idx}",
                        'arxiv_url': '',
                        'pdf_url': '',
                        'word_count': len(str(row.get('Title-é¢˜å', '')).split()) * 20,
                        'published': f"{row.get('Year-å¹´', 2024)}-01-01" if pd.notna(row.get('Year-å¹´')) else '2024-01-01'
                    }
                    
                    # Classify the paper
                    paper['category'] = enhanced_classification_for_cnki(
                        paper['title'],
                        paper['keywords'],
                        paper['category_code']
                    )
                    
                    # Create abstract if missing
                    if not paper.get('abstract', '') and paper['keywords']:
                        paper['abstract'] = f"Research Topic: {', '.join(paper['keywords'][:5])}. Published in {paper['source']} ({paper['year']})."
                    
                    all_papers.append(paper)
            
            elif 'å¯¼å¸ˆ' in sheet_df.columns:  # Thesis format
                for idx, row in sheet_df.iterrows():
                    if pd.isna(row.get('Title-æ–‡çŒ®é¢˜å')):
                        continue
                    
                    keywords_str = str(row.get('å…³é”®è¯', ''))
                    keywords = []
                    if isinstance(keywords_str, str):
                        keywords = [kw.strip() for kw in keywords_str.split(';;') if kw.strip()]
                    
                    paper = {
                        'title': str(row.get('Title-æ–‡çŒ®é¢˜å', '')),
                        'authors': [str(row.get('Author-ä½œè€…', '')).strip()],
                        'source': str(row.get('Source-æ–‡çŒ®æ¥æº', '')),
                        'year': int(row.get('Year-å­¦ä½å¹´åº¦', 2024)) if not pd.isna(row.get('Year-å­¦ä½å¹´åº¦')) else 2024,
                        'keywords': keywords,
                        'category_code': str(row.get('ä¸­å›¾åˆ†ç±»å·', '')),
                        'type': 'thesis',
                        'abstract': f"å­¦ä½è®ºæ–‡: {row.get('Source-æ–‡çŒ®æ¥æº', '')} - å¯¼å¸ˆ: {row.get('å¯¼å¸ˆ', '')}",
                        'arxiv_id': f"THESIS_{sheet_name}_{idx}",
                        'arxiv_url': '',
                        'pdf_url': '',
                        'word_count': len(str(row.get('Title-æ–‡çŒ®é¢˜å', '')).split()) * 80,
                        'published': f"{row.get('Year-å­¦ä½å¹´åº¦', 2024)}-01-01" if pd.notna(row.get('Year-å­¦ä½å¹´åº¦')) else '2024-01-01',
                        'advisor': str(row.get('å¯¼å¸ˆ', ''))
                    }
                    
                    paper['category'] = enhanced_classification_for_cnki(
                        paper['title'],
                        paper['keywords'],
                        paper['category_code']
                    )
                    
                    all_papers.append(paper)
        
        return all_papers
        
    except Exception as e:
        st.error(f"Error loading Excel file: {e}")
        return []

# ==================== LOAD RESEARCH PAPERS ====================
@st.cache_data
def load_research_papers():
    """Load research papers from both JSON and Excel sources"""
    all_papers = []
    
    # Load from JSON if exists
    json_path = 'research_papers.json'
    if os.path.exists(json_path):
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                json_papers = json.load(f)
                all_papers.extend(json_papers)
        except:
            pass
    
    # Load from Excel CNKI
    excel_path = 'CNKI-20260104152201560.xls'
    if os.path.exists(excel_path):
        excel_papers = load_excel_data(excel_path)
        if excel_papers:
            all_papers.extend(excel_papers)
    
    # If no papers loaded, create sample data
    if not all_papers:
        st.warning("No papers loaded. Using sample data.")
        all_papers = [
            {
                'title': 'Sample: Green Finance Development in China',
                'authors': ['Zhang Wei', 'Li Ming'],
                'source': 'Finance Research',
                'year': 2024,
                'keywords': ['green finance', 'sustainable development', 'ESG'],
                'category': 'Green Finance',
                'type': 'journal',
                'abstract': 'A study on green finance development in China.',
                'arxiv_id': 'SAMPLE_001',
                'word_count': 5000,
                'published': '2024-01-15'
            }
        ]
    
    # Convert to DataFrame
    papers_df = pd.DataFrame(all_papers)
    
    # Process dates
    if 'published' in papers_df.columns:
        papers_df['published_date'] = pd.to_datetime(papers_df['published'], errors='coerce')
        papers_df['date_display'] = papers_df['published_date'].dt.strftime('%b %d, %Y')
    
    if 'year' not in papers_df.columns and 'published_date' in papers_df.columns:
        papers_df['year'] = papers_df['published_date'].dt.year.fillna(2024)
    
    # Add colors for categories
    category_colors = {
        'Computational Finance': '#667eea',
        'Mathematical Finance': '#f59e0b',
        'Portfolio Management': '#10b981',
        'Risk Management': '#8b5cf6',
        'Green Finance': '#22c55e',
        'Climate Finance': '#0ea5e9',
        'Sustainable Finance': '#10b981',
        'FinTech & Blockchain': '#6366f1',
        'Banking & Financial Institutions': '#8b4513',
        'Corporate Finance': '#4169e1',
        'General Finance': '#764ba2',
        'Unknown': '#94a3b8'
    }
    
    papers_df['category_color'] = papers_df['category'].map(category_colors).fillna('#94a3b8')
    
    return papers_df, all_papers

# ==================== HEADER ====================
st.title("ğŸ“ˆ Finance Research Hub")
st.markdown("Discover, classify, and explore cutting-edge finance research papers")

# ==================== LOAD DATA ====================
with st.spinner("Loading research papers..."):
    papers_df, papers_list = load_research_papers()

# ==================== RESEARCH LIBRARY ====================
def display_research_library():
    """Display the research library interface"""
    
    st.header("ğŸ“š Research Library")
    st.markdown("Browse and explore finance research papers from arXiv and CNKI")
    
    if not papers_df.empty:
        cnki_papers = len(papers_df[papers_df['arxiv_id'].str.startswith('CNKI') | 
                                    papers_df['arxiv_id'].str.startswith('THESIS')])
        other_papers = len(papers_df) - cnki_papers
        
        # Stats
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Total Papers", len(papers_df))
        with col2:
            st.metric("Categories", papers_df['category'].nunique())
        with col3:
            st.metric("Chinese Papers", cnki_papers)
        with col4:
            st.metric("Other Papers", other_papers)
    
    # Search and filter
    with st.expander("ğŸ” Search & Filter Papers", expanded=True):
        search_cols = st.columns([3, 1, 1])
        with search_cols[0]:
            search_query = st.text_input("Search papers", placeholder="Type keywords...")
        
        with search_cols[1]:
            if 'category' in papers_df.columns:
                categories = ["All"] + sorted(papers_df['category'].dropna().unique().tolist())
                selected_category = st.selectbox("Category", categories)
        
        with search_cols[2]:
            if 'year' in papers_df.columns:
                years = ["All"] + sorted(papers_df['year'].dropna().unique().tolist(), reverse=True)
                selected_year = st.selectbox("Year", years)
    
    # Filter papers
    filtered_df = papers_df.copy()
    
    if search_query:
        mask = (
            filtered_df['title'].str.contains(search_query, case=False, na=False) |
            filtered_df['abstract'].str.contains(search_query, case=False, na=False)
        )
        filtered_df = filtered_df[mask]
    
    if selected_category != "All":
        filtered_df = filtered_df[filtered_df['category'] == selected_category]
    
    if selected_year != "All":
        filtered_df = filtered_df[filtered_df['year'] == int(selected_year)]
    
    # Display papers
    if filtered_df.empty:
        st.info("No papers found matching your criteria.")
    else:
        st.subheader(f"Found {len(filtered_df)} papers")
        
        for idx, paper in filtered_df.iterrows():
            with st.container():
                col1, col2 = st.columns([4, 1])
                
                with col1:
                    st.markdown(f"**{paper.get('title', 'Untitled')}**")
                    st.markdown(f"ğŸ‘¥ {', '.join(paper.get('authors', [])) if isinstance(paper.get('authors'), list) else paper.get('authors', 'Unknown')}")
                    
                    if paper.get('keywords'):
                        keywords = paper['keywords'][:5] if isinstance(paper['keywords'], list) else []
                        if keywords:
                            st.markdown(f"ğŸ·ï¸ **Keywords:** {', '.join(keywords)}")
                    
                    if paper.get('abstract'):
                        with st.expander("Abstract"):
                            st.write(paper['abstract'])
                
                with col2:
                    st.markdown(f"<div style='background-color: {paper.get('category_color', '#e0e7ff')}20; padding: 8px; border-radius: 8px; border: 1px solid {paper.get('category_color', '#e0e7ff')}80;'>"
                                f"<small>{paper.get('category', 'Unknown')}</small><br>"
                                f"<small>ğŸ“… {paper.get('date_display', 'Unknown')}</small>"
                                f"</div>", unsafe_allow_html=True)
                
                st.divider()

# ==================== ENHANCED CLASSIFIER ====================
def display_enhanced_classifier():
    """Display the enhanced classifier interface"""
    
    st.header("ğŸ¤– Bilingual AI Classifier")
    st.markdown("Classify finance papers using bilingual keyword analysis")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        paper_title = st.text_area("Paper Title", placeholder="Enter the research paper title...", height=60)
        paper_abstract = st.text_area("Abstract / Summary", placeholder="Paste the abstract or summary...", height=200)
        
        col_opt1, col_opt2 = st.columns(2)
        with col_opt1:
            top_k = st.slider("Number of categories", 3, 10, 5)
        with col_opt2:
            min_confidence = st.slider("Minimum confidence (%)", 10, 100, 20)
        
        if st.button("ğŸš€ Run Bilingual Classification", type="primary", use_container_width=True):
            if paper_title or paper_abstract:
                with st.spinner("Classifying..."):
                    time.sleep(1)
                    combined_text = f"{paper_title} {paper_abstract}"
                    results = enhanced_classify_with_confidence(combined_text, top_k)
                    
                    # Filter by confidence
                    filtered_results = [r for r in results if r['confidence'] >= min_confidence]
                    
                    if filtered_results:
                        top_result = filtered_results[0]
                        
                        # Display top result
                        st.subheader("ğŸ¯ Primary Classification")
                        col_a, col_b = st.columns([3, 1])
                        
                        with col_a:
                            st.markdown(f"### {top_result['icon']} {top_result['category']}")
                            if top_result['matched_keywords']:
                                st.markdown(f"**Matched keywords:** {', '.join(top_result['matched_keywords'][:5])}")
                        
                        with col_b:
                            confidence_color = "green" if top_result['confidence'] > 70 else "orange" if top_result['confidence'] > 40 else "red"
                            st.markdown(f"<h1 style='color: {confidence_color}; text-align: center;'>{top_result['confidence']:.1f}%</h1>", unsafe_allow_html=True)
                            st.progress(top_result['confidence'] / 100)
                        
                        # Display all results
                        st.subheader("ğŸ“Š All Category Scores")
                        cols = st.columns(min(len(filtered_results), 4))
                        
                        for idx, result in enumerate(filtered_results):
                            with cols[idx % len(cols)]:
                                st.markdown(f"""
                                <div style='border: 1px solid #e0e0e0; border-radius: 10px; padding: 15px; text-align: center;'>
                                    <div style='font-size: 24px;'>{result['icon']}</div>
                                    <div style='font-weight: bold;'>{result['category']}</div>
                                    <div style='font-size: 20px; color: {result["color"]};'>{result['confidence']:.1f}%</div>
                                    <div style='font-size: 12px; color: gray;'>{result['total_matches']} matches</div>
                                </div>
                                """, unsafe_allow_html=True)
                    else:
                        st.warning(f"No categories found with confidence â‰¥ {min_confidence}%")
            else:
                st.error("Please enter at least a title or abstract.")
    
    with col2:
        st.subheader("ğŸ“š Bilingual Database")
        st.metric("Categories", len(FINANCE_KEYWORD_DATABASE))
        
        # Show database info
        with st.expander("View Categories"):
            for category, data in FINANCE_KEYWORD_DATABASE.items():
                st.markdown(f"**{data['icon']} {category}**")
                st.caption(f"{len(data['keywords'])} keywords")

# ==================== STATISTICS DASHBOARD ====================
def display_statistics():
    """Display statistics dashboard"""
    
    st.header("ğŸ“Š Research Analytics")
    st.markdown("Insights and trends from the bilingual research collection")
    
    if papers_df.empty:
        st.warning("No data available for analytics.")
        return
    
    # Basic stats
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("Total Papers", len(papers_df))
    with col2:
        st.metric("Chinese Papers", len(papers_df[papers_df['arxiv_id'].str.contains('CNKI|THESIS', na=False)]))
    with col3:
        st.metric("Categories", papers_df['category'].nunique())
    with col4:
        st.metric("Average Year", int(papers_df['year'].mean()) if 'year' in papers_df.columns else 2024)
    
    # Charts
    col_chart1, col_chart2 = st.columns(2)
    
    with col_chart1:
        st.subheader("Category Distribution")
        if 'category' in papers_df.columns:
            category_counts = papers_df['category'].value_counts()
            fig = px.pie(
                values=category_counts.values,
                names=category_counts.index,
                hole=0.4,
                color_discrete_sequence=px.colors.qualitative.Set3
            )
            st.plotly_chart(fig, use_container_width=True)
    
    with col_chart2:
        st.subheader("Publication Trend")
        if 'year' in papers_df.columns:
            yearly_counts = papers_df['year'].value_counts().sort_index()
            fig = px.bar(
                x=yearly_counts.index,
                y=yearly_counts.values,
                labels={'x': 'Year', 'y': 'Number of Papers'}
            )
            st.plotly_chart(fig, use_container_width=True)
    
    # Data table
    with st.expander("View Raw Data"):
        st.dataframe(papers_df[['title', 'authors', 'category', 'year', 'source']].head(20))

# ==================== SIDEBAR ====================
with st.sidebar:
    st.title("ğŸ“ˆ Finance Research Hub")
    st.markdown("v4.0 â€¢ Bilingual Edition")
    
    st.divider()
    
    # Navigation
    page = st.radio(
        "Navigation",
        ["ğŸ“š Research Library", "ğŸ¤– Enhanced Classifier", "ğŸ“Š Analytics"]
    )
    
    st.divider()
    
    # File check
    check_files()
    
    st.divider()
    
    # Quick actions
    if st.button("ğŸ”„ Clear Cache & Reload"):
        st.cache_data.clear()
        st.rerun()
    
    # Info
    if not papers_df.empty:
        st.caption(f"ğŸ“„ {len(papers_df)} papers loaded")
        if 'category' in papers_df.columns:
            st.caption(f"ğŸ“Š {papers_df['category'].nunique()} categories")

# ==================== MAIN APP ====================
if page == "ğŸ“š Research Library":
    display_research_library()
elif page == "ğŸ¤– Enhanced Classifier":
    display_enhanced_classifier()
elif page == "ğŸ“Š Analytics":
    display_statistics()

# ==================== FOOTER ====================
st.divider()
st.caption("Finance Research Hub â€¢ v4.0 â€¢ Bilingual Edition â€¢ Made for researchers")